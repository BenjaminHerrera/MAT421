{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7OR0-qQw0eWH"
      },
      "source": [
        "# ⚠️ EDIT \"OPEN IN COLAB\" BADGE PRIOR TO DOING ASSIGNMENT"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<a target=\"_blank\" href=\"https://colab.research.google.com/github/BenjaminHerrera/MAT421/blob/main/herrera_module_E.ipynb\">\n",
        "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
        "</a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# **MODULE E:** Linear Equations and Regression\n",
        "# **AUTHOR:** Benjamin Joseph L. Herrera\n",
        "# **CLASS:** MAT 421\n",
        "# **DATE:** 11 FEB 2024"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## What is Linear Algebra?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Linear algebra is focused on the calculation and representation of numbers stored in vectors and matrixes. This field of mathematics allows us to calculate structured data easily and find solutions to different problems represented as these data formats. Niches of computation that use linear algebra are machine learning (from linear regression to key, query, and value matrixes in self-attention models), computer hardware (e.g., hamming codes in matrixes), and efficient information retrieval (e.g. vector-base databases). "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Linear Combination"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Linear combinations are essentially the summation of products between vector and a constant. This in return can form a linear subspace. A linear subspace is defined as a subset $A$ of a linear subspace $B$ where every element inside $A$ can be added onto another element and can be scaled by some scalar $a$, while still being part of $A$:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "$$a_1, a_2 \\isin A$$\n",
        "$$\\beta \\isin \\Reals$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "$$a_1 + a_2 \\isin A$$\n",
        "$$\\beta a_1 \\isin A$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "A span is a area of set of vectors where all possible linear combinations of the vectors are inside the linear subspace $A$. It is also a linear subspace by itself, as well. A span of some set of vectors $v_1, v_2, v_3, \\dots, v_m \\isin V$ can be defined as:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "$$\\textrm{span}(v_1, \\dots, v_m) = \\lbrace\\sum_{i=1}^m {\\beta_iw_i} \\textrm{ : } {\\beta_1, \\dots, \\beta_m \\isin \\Reals}\\rbrace$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Given a matrix $A \\isin \\Reals^{n\\times m}$, the column space of a A is noted as $\\textrm{col(A)}$. The column space of $A$ can be seen as: $\\textrm{col}(A) = \\textrm{span}(a_1, \\dots, a_m) \\isin \\Reals^n$ where the vectors inside the argument of $span(\\centerdot)$ is the columns of $A$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Linear Independence and Dimension"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can identify whether or not a set of vectors $U$ has no redundant information via the concept of linear independence. This is calculated by the following:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "$$u_i \\in U$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "$$\\forall i, u_i \\notin \\textrm{span}({u_j : j\\neq i})$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This means that to achieve linear independence, no vectors in set $U$ can be a linear combination of each other. This is useful because we can use this concept to determine if there is any redundant information in our set of vectors (or matrix in that fact)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "With this concept in mind, we can formulate a basis. A basis is essentially the minimalistic representation of a given subspace. A vector space can have several different bases, but they all have the same number of elements. This cardinality is called the dimension of the vector space which can be denotead as $\\textrm{dim}(U)$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Orthonormal Bases"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The concept orthogonality states that two vectors or matrixes are orthogonal if they are perpendicular to each other. To find if two items are orthonormal, let's define the Norm and the Inner Product of two vectors. Thhe norm of a vector is defined as: $||u|| = \\sqrt{\\sum_1^nu_i^2}$. The inner product (a.k.a. dot product) is defined as: $<u, v> = u \\cdot v = \\sum_i^n u_i v_i$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "To find if two vectors are orthonormal, we simply get the dot product of those two vectors and check if it results to zero. If so, then the vectors are orthonormal."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "If a given set of vectors is orthonormal, it means that each vector in that set are orthogonal to each other:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "$$j \\neq i$$\n",
        "$$<u_i, u_j> = 0$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "and each have a norm of 1:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "$$||u_i|| = 1$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Best Approximation Theorem"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's take a look at a problem. Given a linear subspace $U \\subseteq V$ and a vector $a \\notin U$, we want to find a vector $a* \\in U$ where:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "$$\\textrm{min}_{a* \\in U}||a* - a||$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This problem statement is the best generalization for many optimization application. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's define an orthogonal projection. Let the orthonormal basis of $U$ be $q_1, \\dots, q_m$. The projection of $a$ onto $U$ is:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "$$\\sum_{j=1}^m<v, q_j> qj.$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Gram-Schmidt Process"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The Gram-schmidt process is used to get an orthogonal basis of a linear subspace $A$, $q_1, \\dots, q_m$. We can calculate each value of $q_m$ via the following equation:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "$$q_m = a_m - \\sum_{j=1}^{m-1}\\textrm{proj}_{q_j}(a_m)$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "where $a_m \\isin A$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Eigenvalues and Eigenvectors"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "An Eigenvalue of a matrix $A$ is a scalar in $\\Reals$ where if multiplied to an eigenvector, it would equal to the product between $A$ and the eigenvector as defined below:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "$$Ax = \\lambda x$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This only applies to matrixes that are square. Additionally, a matrix $A \\isin \\Reals^{d \\times d}$ has a maximum of $d$ unique eigenvalues."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Another thing to note is that if the definition of $A$ remains the same and there are a set of eigenvalues of A $\\lambda_1, \\dots, \\lambda_m$ with eigenvectors that are non-zero $v_1, \\dots, v_m$, then these eigenvectors are linearly independent."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Diagonalization of Symmetric Matrices"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Given a diagonal matrix $D$ of unique entries, then $P$ is a nonsingular matrix P where $A = PDP^{-1}$. This in turn results in the equivalence: $AP = PD$ which later equates to $Ap_i = \\lambda_i p_i$ where $p_i \\in \\textrm{col}(A)$. This means that $\\lambda_i$ is an eigenvalue and $p_i$ would be an eigenvector. In this thought, if $A$ is symmetric, then any pair of eigenvectors for a anotther eigenspace are orthogonal to each other."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "What makes a matrix that is square symmetrical? Here are the following requirements:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- The matrix has $dim(A)$ eigenvalues.\n",
        "- If $\\lambda$ is divisible by $k$, then the eigenspace of the matrix has a dimension of $k$.\n",
        "- The eigenspaces of the matrix are orthogonal to each other.\n",
        "- $A$ can diagonized in an orthogonal manner."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Linear Regression"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Using concepts of linear algebra, we can figure out the best fit function to describe a given set of data (a.k.a. linear regression)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "To solve linear least square problems, we first use the Gram-Schmidtt process to get an orthonormal basis $V$ from the span of a matrix $A$. To get a QR decomposition of $A$, we define a matrix $Q$ where it is the same size as $A$. $Q$ has the properties where multiplying itself to its transpose results in an Identity matrix of size $m \\times m$. The result from the Gram-Schmidt process results in the composition of $A = QR$. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The least squares problem is defined as the minimization of $||Ax - b||$ where $x \\in \\Reals^m$ such that the $A \\in \\Reals^{n \\times m}$ and where $n > m$. To do so, we represent elements of $A$ as $a_{n,m}$ where $n$ is the height index of the elementt and $m$ is the width index of the element. $b$ is represented as a vector."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We then find a linear combination of the columns of $A$ that minimizes the value of $\\sum_{i=1}^n(y_i - b_i)^2$. The value of $y_i$ is defined as $y_i = \\sum^m_{j=1}x_ja_{i,j}$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Afterwards, we calculate a $\\hat{b} = \\textrm{proj}_{\\textrm{col(A)}} b$ where $\\hat{b}$ is the orthogonal projection of $\\textrm{col}(A)$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This will lead to a consistent $A\\hat{x} = \\hat{b}$ where $A$ is the solution to the the problem."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyNwKPny2Im792IMMHKruYCG",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
