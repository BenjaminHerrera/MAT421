{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<a target=\"_blank\" href=\"https://colab.research.google.com/github/BenjaminHerrera/MAT421/blob/main/herrera_module_F.ipynb\">\n",
        "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
        "</a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# **MODULE F:** Differentiation and Optimization\n",
        "# **AUTHOR:** Benjamin Joseph L. Herrera\n",
        "# **CLASS:** MAT 421\n",
        "# **DATE:** 18 FEB 2024"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Limits and Continuity"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We use limits to calculate at what value does a function approach to a given input value. This is useful for determining continuity, integrals, and derivatives. We formally define this as the following."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Given a function $f : D \\rightarrow \\Reals$ where $D$ is a subset of $\\Reals^d$, $f$ has a limit $L \\in \\Reals$ when $x$ in $f(x)$ approaches some value $a \\in \\Reals$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We also define an $r$-ball around $x \\in \\Reals^d$:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "$$B_r(x) = \\{z \\in \\Reals^d : ||z-x|| < r\\}$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This $r-ball$ is a essentially a function that returns vectors in $\\Reals^d$ that are within the vicinity of $x$ around $r$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Going back to our definition of a limit, we use the $r$-ball to restrict $x$ where $x \\in D \\cap B_\\sigma(a)$ and $\\sigma > 0$. Given this definition of zero, the limit of $f$ is accepted if $|f(x) - L| < \\epsilon$ where \\epsilon > 0. This essentially means that a limit is accepted if it doesn't exceeds a certain error threshold."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "As we all know, continuity is essentially the notion that a function does not have \"skipped\" $f(x)$ values $\\forall x$. We can define the occurrence of a discontinuity, the opposite of a continuity, as $f(x_*) \\neq L$ where $x_*$ is the point where the discontinuity occurs."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Another thing to note is that if $f : D_1 \\rightarrow \\Reals^m$ is continuous at $x_i$ and $g : D_2 \\rightarrow \\Reals^n$ is continuous when $f(x_i)$, then $g \\circ f$ is continuous at $x_i$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "There are also minimums and maximums for a function $f : D \\rightarrow \\Reals$. Maximum is defined when $f(x^*)$ is maximum $M$ and $M \\geq f(x)$, $\\forall x \\in D$. Minimum is defined when $f(x^{\\circ})$ is minimum $N$ and $N \\leq f(x)$, $\\forall x \\in D$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Derivatives"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Derivatives are simply defined as the rate of change of a function with respect to a given time. This is defined as the function below:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "$$f'(x) = \\frac{df}{dx} = \\lim_{h \\rightarrow 0} \\frac{f(x + h) - f(x)}{h}$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Given a function $f : D \\rightarrow \\Reals$ and a value $x, x^* \\in D$, we can find extreme values of $f$ when $f'(x^*) > 0$ at $x$ if $f(x) > f(x^*)$ when $x > x^*$; or if $f(x) < f(x^*)$ when $x < x^*$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Another thing to note is that if the defined function has a domain range of $[a, b]$ and is continuous, if $f(a) = f(b)$, then there exists a $d$ between the domain range where $f'(d) = 0$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can aptly define another notion with this notion in another way via the following equation:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "$$f'(d) = \\frac{f(b) - f(a)}{b - a}$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "It's simply the rise-over-run-equals-the-slope formula! The derivative of $f$ at step $d$, in a way, is define with this simple equation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "With the given definitions and notions above, we can also do a derivative of the derivative itself with the following equation:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "$$f''(x) = \\frac{d^2f}{dx^2} = \\lim_{h \\rightarrow 0} \\frac{f'(x + h) - f'(x)}{h}$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now a function can have multiple different inputs like $f(x, y, z) = xy + z - z^2$. This can be challenging to derive the function on all inputs, but we can derive at one at a time. Say hello to partial derivatives! This can be simply be defined below:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "$$\\frac{\\partial f(x)}{ \\partial x_i} = \\lim_{h \\rightarrow 0} \\frac{f(x + he_i)- f(x)}{h}$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "where $e_i$ is a basis vector in the $i$-th position and $e_i \\in \\Reals^d$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can find the rate of change of a function over each variables via the Jacobian. Essentially, the Jacobian of a function ($J_f(x)$) returns a matrix of size $d \\times n$ where $n$ is the number of inputs that a function $f$ takes. Each element in the matrix with their corresponding position $i, j$ is essentially $\\frac{\\partial f_n(x)}{ \\partial(x_d)}$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This is useful for finding all of the interactions of the inputs with each other when deriving a multi-variable function $f$. We also call this the gradient of $f$ as $\\nabla f(x)$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "When a composition $g \\circ f$ exists, the Jacobian of that composition is:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "$$J_{g \\circ f}(x) = J_g(f(x)) \\cdot J_f(x)$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This is similar to chain rule!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now if we want to take the directional derivative of a function $f$, we get a unit direction vector $v \\in \\Reals^d$ and find the derivative of that function via:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "$$\\frac{\\partial f(x)}{ \\partial v} = \\lim_{h \\rightarrow 0} \\frac{f(x + hv)- f(x)}{h}$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Pretty similar to the previous definition of partial derivatives explained above."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now, if we want to get the gradient of the function with respect to the directional vector, we calculate via the following:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "$$\\frac{\\partial f(x)}{\\partial v} = J_f(x) \\cdot v = \\nabla f(x)^T \\cdot v$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "What if we want to do our partial derivatives to the higher order? Well, we can define that via:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "$$\\frac{\\partial^2 f(x)}{ \\partial x_n \\partial x_m} = \\lim_{h \\rightarrow 0} \\frac{\\partial f(x + he_n) / \\partial x_m - \\partial f(x) / \\partial x_m }{h}$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The position of the variables being partially derived by do not matter in order:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "$$\\frac{\\partial^2 f(x)}{ \\partial x_n \\partial x_m} = \\frac{\\partial^2 f(x)}{ \\partial x_m \\partial x_n}$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Taylor's Theorem"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Taylor's Theorem allows us to get a rough estimation of a function around $x$ via the form of a polynomial. Using taylor's theorem and the early definitions of derivatives explained above, we can extrapolate or estimate the definition of a function $f(z)$ on the domain $[x, z]$ via the form:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "$$f(z) = f(x) + (z-b)f'(x) + \\dots \\frac{(z - x)^{n - 1}}{(n - 1)!}f^{n-1}(x) + C$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Where $C$ is defined as $\\frac{(z - x)^{n}}{n!} f^n(x + \\sigma(z - m))$ where $\\sigma$ is a value between $(0, 1)$ ."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "So if our precision is up to two degrees, $n=2$, the the equation would look like this:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "$$f(z) = f(x) + (z-b)f'(x) + \\frac{1}{2}(z - x)^2f''(\\delta)$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We also define the Multivariate Mean Value of a function $f$ as:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "$$f(x) = f(x^{\\circ}) + \\nabla f(x^{\\circ} + \\delta(x-x^{\\circ}))^T \\cdot (x-x^{\\circ})$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Conditions for Local Minimizers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "A minimizer takes in the form:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "$$\\min_{x \\in \\Reals^d}f(x)$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We define a global minimizer as the following. Given a function $f : \\Reals^d \\rightarrow \\Reals$, a point $a$ is the minimum such that $f(x) \\geq f(a)$ for all $x$ in $\\Reals^d$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "On the other hand, a local minimizer is defined as the same as above, but instead for all $x$ in $B_\\delta(a)$. Pretty much meaning what is the lowest value around $a$ within $\\delta$ radius."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now a descent direction is defined as a vector $v$, with the same definition of the function $f$ above, as: "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "$$\\frac{\\partial f(x)}{\\partial v} = \\nabla f(x)^T \\cdot v < 0$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can tell if a function has a descent direction $a$ if $\\nabla f(a) \\neq 0$. If not, then there is nowhere to descend, meaning it's the minima for the area. This is known as the first-order necessary condition for the minimizer of a function. This also means that the hessian of the function at point $a$ is positive semi-definite (second-order necessary condition)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We talked about necessary conditions, let's look at sufficient conditions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "For $a$ to be a strict local minima for a function $f$, same definition as above, it must have the gradient of the function $a$ be 0 and the hessian of the function at $a$ be positive definite. This is called the second-order sufficient condition. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Convexity and Global Minimizers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "A convex function is essentially a function where a line is above any point on a function."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We define a convex set as a set $C \\subseteq \\Reals^d$ where $\\forall x, y \\in C$ and $\\beta \\in [0, 1]$ satisfy the notion of $(1 - \\beta) \\cdot x + \\beta \\cdot y \\in D$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Similarly, a convex function is defined as $f((1 - \\beta) \\cdot x + \\beta \\cdot y) \\leq (1 - \\beta) \\cdot f(x) + \\beta \\cdot f(y)$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "A first-order convexity condition for a convex function $f$ follows the notion where $f(y) \\geq f(x) + \\nabla f(x)^T (y - x)$. Similarly to second order conditions for minimizers, the second-order convexity condition for a convex function is defined has the fact that a function $f$ is convex iff the hessian of $f$ at point $a$ is positive semi-definite."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now, combining the fact that if $f$ is a convex function, if $\\nabla f(x) = 0$, this means that at point $x$, then it is the minimizer for $f$. But since $f$ is a convex function, then $a$ is the global minimizer for $f$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Consequently, any local minimizer of $f$ is a global minimizer!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Gradient Descent"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Given the understanding of derivatives, gradients, minimum definitions, and minimums of convex functions, we can calculate a way to find the minimum of a function. This is known as gradient descent. This operation is useful for the core aspects of machine learning where we want to minimize a cost function $f(x)$:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "$$\\min_{x \\in \\Reals^d}f(x)$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "To find the direction of where to descend with the greatest descent, we find a descent vector $v$ that satisfies:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "$$\\frac{\\partial f(x)}{\\partial v} \\geq \\frac{\\partial f(x)}{\\partial v^{\\circ}}$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "where $v^{\\circ}$ is defined as $-\\frac{\\nabla f(x)}{||\\nabla f(x)||}$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Essentially, we are finding a direction that has a greater change in step that all the other possible descents by looking at the gradients and their path."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyNwKPny2Im792IMMHKruYCG",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
